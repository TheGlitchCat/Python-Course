{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Style\n",
    "\n",
    "<img src=\"https://shafeentejani.github.io/assets/images/fast_style_transfer/gatys_results.png\">\n",
    "\n",
    "\n",
    "\n",
    "- Example Based vs Collection Based\n",
    "- Collection Based:\n",
    "    - Paired\n",
    "    - Unpaired\n",
    "- Common Tool:\n",
    "    GAN ( Generative Adversarial Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*fd3QUV45REqZ_f7tjYs31g.png\">\n",
    "\n",
    "## GANs\n",
    "Architecture:\n",
    "- Generative Network\n",
    "- Discriminatory Network\n",
    "- Zero-sum game (opposite losses)\n",
    "\n",
    "<img src=\"https://pathmind.com/images/wiki/GANs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss of GANs\n",
    "- Discriminatory Network: cross entropy when the network should reject\n",
    "$$ L_{D} ~ -\\log(D(x)) + -\\log(1 - D(G(z))) $$\n",
    "where $D(x) = \\text{Real Data} \\text{ and } D(z) = \\text{Fake Data}$\n",
    "\n",
    "- Generative Network: inverse of the discriminative (zero-sum game)\n",
    "$$ L_{G} ~ C + \\log(1 - D(G(z))) \\text{ or } - \\log(D(G(z))) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN\n",
    "\n",
    "<img src=\"https://junyanz.github.io/CycleGAN/images/teaser.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Repository Source](https://github.com/aitorzip/PyTorch-CycleGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[datasets - fix for windows](https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        conv_block = [\n",
    "            nn.ReflectionPad2d(1), # Better Padding\n",
    "            nn.Conv2d(in_features, in_features, 3), #Kernel_size = 3\n",
    "            nn.InstanceNorm2d(in_features), # Contrast in images and regularization / BN for GANs\n",
    "            nn.ReLU(True),\n",
    "            nn.ReflectionPad2d(1), # preserves images distribution\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        ]\n",
    "        \n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "        #self.conv_block = conv_block\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x) + x # Increase Models Space    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc,  n_residual_blocks = 9):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Concolutional block\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(input_nc, 64, 7), # I - 7(filter) + 6(2*padding) / 1 +1 = I\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(True)\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        \n",
    "        in_features = 64\n",
    "        out_features = in_features * 2  # Amount of channels\n",
    "        \n",
    "        # Encoding\n",
    "        # Multiply by 2 number of channels\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2,padding=1 ), # I / 2\n",
    "                nn.InstanceNorm2d(out_features), \n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "            \n",
    "            in_features  = out_features\n",
    "            out_features = in_features*2 \n",
    "        \n",
    "        # Residuals transforms\n",
    "        \n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)] # 9 Residual layers\n",
    "        \n",
    "        # Decoding\n",
    "        out_features = in_features // 2\n",
    "        \n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, 3 , \n",
    "                                   stride=2, padding=1, output_padding=1), # 2I also Mat=gradient\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            \n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "\n",
    "        # output\n",
    "        \n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, output_nc, 7), # I\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "        #self.model = model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PatchGAN\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, 64, 4, stride=2, padding=1), # I / 2\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1), # I / 2\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1), # I / 2\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv2d(256, 512, 4, padding=1), # I - 1\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        ]\n",
    "        \n",
    "        # Flatten\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv2d(512, 1, 4, padding=1) # I - 1 \n",
    "        ]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "        #self.model = model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os \n",
    "import itertools\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size=50):\n",
    "        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0,1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size-1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return Variable(torch.cat(to_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, base_dir, transform=None, split='train'):\n",
    "        self.transform = transforms.Compose(transform)\n",
    "        self.files_A = sorted(glob.glob(os.path.join(base_dir, '{}A/*.*'.format(split))))\n",
    "        self.files_B = sorted(glob.glob(os.path.join(base_dir, '{}B/*.*'.format(split))))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_A = self.transform(Image.open(self.files_A[idx]))\n",
    "        image_B = self.transform(Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]))\n",
    "        return {'A': image_A, 'B':image_B}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "n_epochs = 200\n",
    "batch_size = 2\n",
    "lr = 0.0002\n",
    "\n",
    "size = 256\n",
    "input_nc = 3\n",
    "output_nc = 3\n",
    "decay_epoch = 100\n",
    "\n",
    "cuda = True\n",
    "n_cpu = 8\n",
    "\n",
    "base_dir = './Datasets/summer2winter_yosemite/'\n",
    "\n",
    "device = torch.device('cuda' if cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inital params of NN\n",
    "def weights_init_normal(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.normal(m.weight.data, 0.0, 0.02)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant(m.bias, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheGlitchCat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "netG_A2B = Generator(input_nc, output_nc)\n",
    "netG_B2A = Generator(input_nc, output_nc)\n",
    "netD_A = Discriminator(input_nc)\n",
    "netD_B = Discriminator(input_nc)\n",
    "\n",
    "\n",
    "netG_A2B.apply(weights_init_normal)\n",
    "netG_B2A.apply(weights_init_normal)\n",
    "netD_A.apply(weights_init_normal)\n",
    "netD_B.apply(weights_init_normal)\n",
    "\n",
    "\n",
    "if cuda:    \n",
    "    netG_A2B.to(device)\n",
    "    netG_B2A.to(device)\n",
    "    netD_A.to(device)\n",
    "    netD_B.to(device)\n",
    "\n",
    "criterion_GAN  = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()), lr=lr,\n",
    "                             betas=(0.5, 0.999))\n",
    "\n",
    "optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=lr, betas=(0.5,0.999))\n",
    "optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=lr, betas=(0.5,0.999))\n",
    "\n",
    "\n",
    "# Schedulers (update the learning rate dynamically during training)\n",
    "\n",
    "class LambdaLR():\n",
    "    def __init__(self, n_epoch, offset, decay_start_epoch):\n",
    "        assert((n_epochs-decay_start_epoch) > 0 )\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "    \n",
    "    def step(self, epoch):\n",
    "        return 1 - max(0, epoch + self.offset - self.decay_start_epoch ) / (self.n_epochs - self.decay_start_epoch)\n",
    "        \n",
    "\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs and  Targets\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "#Tensor = torch.Tensor\n",
    "target_real = Tensor(batch_size).fill_(1.0)\n",
    "target_fake = Tensor(batch_size).fill_(0.0)\n",
    "\n",
    "\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "# Dataloader\n",
    "\n",
    "# Data Augmentation\n",
    "transform = [transforms.Resize(int(size*1.12), Image.BICUBIC),\n",
    "             transforms.RandomCrop(size),\n",
    "             transforms.RandomHorizontalFlip(),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "            ]\n",
    " \n",
    "dataloader =  DataLoader(ImageDataset(base_dir, transform=transform), \n",
    "                        batch_size=batch_size, shuffle=True, \n",
    "                        num_workers=0, drop_last=True)\n",
    "\n",
    "\n",
    "def Gen_GAN_loss(G, D, real, loss, target_real):\n",
    "    fake = G(real)\n",
    "    pred_fake = D(fake)\n",
    "    L = loss(pred_fake, target_real)\n",
    "    return L, fake\n",
    "    \n",
    "def cycle_loss(G1, G2, real, loss):\n",
    "    recovered = G2(G1(real))\n",
    "    L = loss(recovered, real)\n",
    "    return L\n",
    "\n",
    "def identity_loss(G, real, loss):\n",
    "    same = G(real)\n",
    "    L = loss(same, real)\n",
    "    return L\n",
    "\n",
    "def Disc_GAN_loss(D2, fake2, real2, fake_2_buffer, loss, target_real, target_fake):\n",
    "    pred_real = D2(real2)\n",
    "    loss_D2_real = loss(pred_real, target_real)\n",
    "\n",
    "    fake2 = fake_2_buffer.push_and_pop(fake2)\n",
    "    pred_fake = D2(fake2.detach())\n",
    "    loss_D2_fake = loss(pred_fake, target_fake)\n",
    "    loss_D2 = (loss_D2_real + loss_D2_fake) * 0.5\n",
    "    \n",
    "    return loss_D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install livelossplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "class Logger():\n",
    "    def __init__(self, n_epochs, batches_epoch, epoch=1):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batches_epoch = batches_epoch\n",
    "        self.epoch = epoch\n",
    "        self.batch = 1\n",
    "        self.prev_time = time.time()\n",
    "        self.mean_period = 0\n",
    "        self.losses = {}\n",
    "        self.loss_windows = {}\n",
    "        self.image_windows = {}\n",
    "\n",
    "\n",
    "    def log(self, losses=None, images=None):\n",
    "        self.mean_period += (time.time() - self.prev_time)\n",
    "        self.prev_time = time.time()\n",
    "\n",
    "        sys.stdout.write('\\rEpoch %03d/%03d [%04d/%04d] -- ' % (self.epoch, self.n_epochs, self.batch, self.batches_epoch))\n",
    "\n",
    "        for i, loss_name in enumerate(losses.keys()):\n",
    "            if loss_name not in self.losses:\n",
    "                self.losses[loss_name] = losses[loss_name].data.item()\n",
    "            else:\n",
    "                self.losses[loss_name] += losses[loss_name].data.item()\n",
    "            \n",
    "            if (i+1) == len(losses.keys()):\n",
    "                sys.stdout.write('%s: %.4f -- ' % (loss_name, self.losses[loss_name]/self.batch))\n",
    "            else:\n",
    "                sys.stdout.write('%s: %.4f | ' % (loss_name, self.losses[loss_name]/self.batch))\n",
    "\n",
    "        batches_done = self.batches_epoch*(self.epoch - 1) + self.batch\n",
    "        batches_left = self.batches_epoch*(self.n_epochs - self.epoch) + self.batches_epoch - self.batch \n",
    "        sys.stdout.write('ETA: %s' % (datetime.timedelta(seconds=batches_left*self.mean_period/batches_done)))\n",
    "\n",
    "        # End of epoch\n",
    "        if (self.batch % self.batches_epoch) == 0:\n",
    "            # Plot losses\n",
    "            for loss_name, loss in self.losses.items():\n",
    "                # Reset losses for next epoch\n",
    "                self.losses[loss_name] = 0.0\n",
    "\n",
    "            self.epoch += 1\n",
    "            self.batch = 1\n",
    "            sys.stdout.write('\\n')\n",
    "        else:\n",
    "            self.batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "logger = Logger(n_epochs, len(dataloader), epoch=epoch)\n",
    "liveloss= PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheGlitchCat\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.58 GiB already allocated; 1.80 MiB free; 2.93 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0db36958222a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mloss_GAN_B2A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake_A\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGen_GAN_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetG_B2A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetD_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_B\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_GAN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_real\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mloss_cycle_ABA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcycle_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetG_A2B\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnetG_B2A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_cycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mloss_cycle_BAB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcycle_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetG_B2A\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnetG_A2B\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_B\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_cycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-f772b0d7e620>\u001b[0m in \u001b[0;36mcycle_loss\u001b[1;34m(G1, G2, real, loss)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcycle_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mrecovered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mG2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecovered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-5fbc01aabeca>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-66ad0b400633>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m \u001b[1;31m# Increase Models Space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.58 GiB already allocated; 1.80 MiB free; 2.93 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch, n_epochs):\n",
    "    print(f'{epoch} / {n_epochs}')\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        #print(i, type(i))\n",
    "        #print(batch, type(batch))\n",
    "        real_A = batch['A'].to(device)\n",
    "        real_B = batch['B'].to(device)\n",
    "        \n",
    "        #real_A = batch['A']\n",
    "        #real_B = batch['B']\n",
    "        \n",
    "        # Generative\n",
    "        \n",
    "        optimizer_G.zero_grad()\n",
    "        loss_GAN_A2B, fake_B = Gen_GAN_loss(netG_A2B, netD_B, real_A, criterion_GAN, target_real)\n",
    "        loss_GAN_B2A, fake_A = Gen_GAN_loss(netG_B2A, netD_A, real_B, criterion_GAN, target_real)\n",
    "        \n",
    "        loss_cycle_ABA = cycle_loss(netG_A2B,netG_B2A, real_A, criterion_cycle)\n",
    "        loss_cycle_BAB = cycle_loss(netG_B2A,netG_A2B, real_B, criterion_cycle)\n",
    "        \n",
    "        loss_identity_A = identity_loss(netG_B2A, real_A, criterion_identity)\n",
    "        loss_identity_B = identity_loss(netG_A2B, real_B, criterion_identity)\n",
    "        \n",
    "        loss_G = (loss_GAN_A2B + loss_GAN_B2A) + 10.0*(loss_cycle_ABA + loss_cycle_BAB) + 5.0 * (loss_identity_A + loss_identity_B) \n",
    "        loss_G.backward()\n",
    "        \n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Discriminatory\n",
    "        optimizer_D_A.zero_grad()\n",
    "        loss_D_A = Disc_GAN_loss(netD_A, fake_A, real_A, fake_A_buffer, criterion_GAN, target_real, target) \n",
    "        loss_D_A.backward()\n",
    "        \n",
    "        optimizer_D_A.step()\n",
    "        \n",
    "        \n",
    "        optimizer_D_B.zero_grad()\n",
    "        loss_D_B = Disc_GAN_loss(netD_B, fake_B, real_B, fake_B_buffer, criterion_GAN, target_real, target) \n",
    "        loss_D_B.backward()\n",
    "        \n",
    "        optimizer_D_B.step()\n",
    "        \n",
    "        log_values = {'loss_G': loss_G,\n",
    "                      'loss_G_identity': (loss_identity_A + loss_identity_B),\n",
    "                      'loss_G_GAN': (loss_GAN_A2B + loss_GAN_B2A),\n",
    "                      'loss_G_cyle': loss_cycle_ABA + loss_cycle_BAB,\n",
    "                      'loss_D': loss_D_A + loss_D_B\n",
    "                 }\n",
    "        logger.log(log_values, images={'real_A': real_A,'real_B': real_B, 'fake_A': fake_A,'fake_B': fake_B  })\n",
    "        \n",
    "    liveloss.update(log_values)\n",
    "    liveloss.draw()\n",
    "    \n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
