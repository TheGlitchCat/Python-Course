{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "RNN Auto Text.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF8AYb4iu1xP",
        "colab_type": "text"
      },
      "source": [
        "# RNN \n",
        "\n",
        "- [original](https://machinetalk.org/2019/02/08/text-generation-with-pytorch/)\n",
        "- [Other Example](https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJXf_ZP9u1xP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "from argparse import Namespace"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQe8mYoau1xT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flags = Namespace(\n",
        "    train_file='./data/Edgar Allan Poe.txt',\n",
        "    seq_size=32,\n",
        "    batch_size=16,\n",
        "    embedding_size=64,\n",
        "    lstm_size=64,\n",
        "    gradients_norm=5,\n",
        "    initial_words=['the', 'raven'],\n",
        "    predict_top_k=5,\n",
        "    checkpoint_path='checkpoint',\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOMsEWb9u1xV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data_from_file(train_file, batch_size, seq_size):\n",
        "    with open(train_file, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    text = text.split()\n",
        "\n",
        "    word_counts = Counter(text)\n",
        "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "    n_vocab = len(int_to_vocab)\n",
        "\n",
        "    print('Vocabulary size', n_vocab)\n",
        "\n",
        "    int_text = [vocab_to_int[w] for w in text]\n",
        "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "    out_text = np.zeros_like(in_text)\n",
        "    out_text[:-1] = in_text[1:]\n",
        "    out_text[-1] = in_text[0]\n",
        "    in_text = np.reshape(in_text, (batch_size, -1))\n",
        "    out_text = np.reshape(out_text, (batch_size, -1))\n",
        "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hONDjAv4u1xX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1P9cKzGu1xZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModule(nn.Module):\n",
        "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "        super(RNNModule, self).__init__()\n",
        "        \n",
        "        #print(f'RNN Module - - {n_vocab} - - {seq_size} - - {embedding_size} - - {lstm_size}')\n",
        "        \n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size,lstm_size,batch_first=True)\n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.dense(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
        "                torch.zeros(1, batch_size, self.lstm_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRdQXTNVu1xc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss_and_train_op(net, lr=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    return criterion, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q5RdYAYu1xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
        "    net.eval()\n",
        "    words = ['the', 'raven']\n",
        "\n",
        "    state_h, state_c = net.zero_state(1)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    \n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "    for _ in range(100):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    print(' '.join(words).encode('utf-8'))\n",
        "    with open(f'./data/predict_text.txt', 'a+') as file:\n",
        "        file.write(str(' '.join(words).encode('utf-8')) + '\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XZdRWrru1xg",
        "colab_type": "code",
        "outputId": "f84f6ba8-683b-46de-a6dd-924655d7ba85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(flags.train_file, flags.batch_size, flags.seq_size)\n",
        "\n",
        "net = RNNModule(n_vocab, flags.seq_size, flags.embedding_size, flags.lstm_size)\n",
        "net = net.to(device)\n",
        "\n",
        "criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
        "\n",
        "iteration = 0\n",
        "\n",
        "for e in range(200):\n",
        "    batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
        "    state_h, state_c = net.zero_state(flags.batch_size)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    \n",
        "    for x, y in batches:\n",
        "        iteration += 1\n",
        "        net.train()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = torch.tensor(x).to(device)\n",
        "        y = torch.tensor(y).to(device)\n",
        "        \n",
        "        \n",
        "        logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
        "        loss = criterion(logits.transpose(1, 2), y)\n",
        "\n",
        "        loss_value = loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        state_h = state_h.detach()\n",
        "        state_c = state_c.detach()\n",
        "\n",
        "        _ = torch.nn.utils.clip_grad_norm_(net.parameters(), flags.gradients_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if iteration % 100 == 0:\n",
        "            print('Epoch: {}/{}'.format(e, 200),'Iteration: {}'.format(iteration),'Loss: {}'.format(loss_value))\n",
        "\n",
        "        if iteration % 1000 == 0:\n",
        "            predict(device, net, flags.initial_words, n_vocab, vocab_to_int, int_to_vocab, top_k=5)\n",
        "            if e == 200 :\n",
        "              torch.save(net.state_dict(),'./data/model-{}.pth'.format(iteration))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 15339\n",
            "Epoch: 0/200 Iteration: 100 Loss: 7.659239292144775\n",
            "Epoch: 1/200 Iteration: 200 Loss: 6.697303295135498\n",
            "Epoch: 2/200 Iteration: 300 Loss: 6.433714389801025\n",
            "Epoch: 3/200 Iteration: 400 Loss: 5.906489372253418\n",
            "Epoch: 3/200 Iteration: 500 Loss: 5.334860324859619\n",
            "Epoch: 4/200 Iteration: 600 Loss: 4.833841800689697\n",
            "Epoch: 5/200 Iteration: 700 Loss: 4.67804479598999\n",
            "Epoch: 6/200 Iteration: 800 Loss: 3.933741807937622\n",
            "Epoch: 7/200 Iteration: 900 Loss: 4.097213268280029\n",
            "Epoch: 7/200 Iteration: 1000 Loss: 3.6593551635742188\n",
            "b\"the raven are stirred no thought--for and certain extent and applicability, by the same time, time for example, of his own exertions at the night waned, or in its author's will, in substance; from his literary labors. in all thy truest type the world of his fellow-students, and even infinitely remote too, to the same time fill that we know no sentience--for it will bring the sea, In those very learned, Her woman's ha! ha!), on which we know no man ever not only in his own views on Samarcand!-- I saw the poet's undying copy Of gorgeous moon. And they will be\"\n",
            "Epoch: 8/200 Iteration: 1100 Loss: 3.542332649230957\n",
            "Epoch: 9/200 Iteration: 1200 Loss: 3.3847601413726807\n",
            "Epoch: 10/200 Iteration: 1300 Loss: 2.9121932983398438\n",
            "Epoch: 11/200 Iteration: 1400 Loss: 2.8456637859344482\n",
            "Epoch: 11/200 Iteration: 1500 Loss: 2.554137706756592\n",
            "Epoch: 12/200 Iteration: 1600 Loss: 2.5076472759246826\n",
            "Epoch: 13/200 Iteration: 1700 Loss: 2.434009313583374\n",
            "Epoch: 14/200 Iteration: 1800 Loss: 2.041163921356201\n",
            "Epoch: 15/200 Iteration: 1900 Loss: 2.1639065742492676\n",
            "Epoch: 15/200 Iteration: 2000 Loss: 1.9534915685653687\n",
            "b'the raven alone, And kindly stars a specimen, broken there is no poet himself. aright? _Duke_. used on my lonely pathway It is needless a growing desire to the world, I will always contrive As having dwelt and hard by a little maiden in any great measure, but in the effect derivable There were you, for a dream of increasing which in this issue, II. From All the sun that you do devote thee in many memories among this immortality, this dire que la young Hope without further increased this very poem, this electronic And beautiful to \"Tamerlane.\" So at all we have'\n",
            "Epoch: 16/200 Iteration: 2100 Loss: 2.0095882415771484\n",
            "Epoch: 17/200 Iteration: 2200 Loss: 1.8253965377807617\n",
            "Epoch: 18/200 Iteration: 2300 Loss: 1.5321910381317139\n",
            "Epoch: 19/200 Iteration: 2400 Loss: 1.6353007555007935\n",
            "Epoch: 19/200 Iteration: 2500 Loss: 1.6171984672546387\n",
            "Epoch: 20/200 Iteration: 2600 Loss: 1.5640511512756348\n",
            "Epoch: 21/200 Iteration: 2700 Loss: 1.3974801301956177\n",
            "Epoch: 22/200 Iteration: 2800 Loss: 1.2118347883224487\n",
            "Epoch: 23/200 Iteration: 2900 Loss: 1.3400033712387085\n",
            "Epoch: 23/200 Iteration: 3000 Loss: 1.3472422361373901\n",
            "b'the raven having never [Footnote 17: fawn For in my angrier tells The wild rose intense eBooks in Rome! high spiritual uses. than any one his translations from all the omission could shall not be carriers O late to reaching his reputation. nor more intense and chilly to what they growe.\"] Than up in thine but gray everything render \"Annabel Lee\" arise As the fact is, that in a single short poet without form the book as of a fiend. flowers during one word, a certain word as I mistake I do make Beauty, that I have not drop them. _Aless_. I said'\n",
            "Epoch: 24/200 Iteration: 3100 Loss: 1.323810338973999\n",
            "Epoch: 25/200 Iteration: 3200 Loss: 1.1988567113876343\n",
            "Epoch: 26/200 Iteration: 3300 Loss: 0.9938316345214844\n",
            "Epoch: 27/200 Iteration: 3400 Loss: 1.1255635023117065\n",
            "Epoch: 27/200 Iteration: 3500 Loss: 1.1067460775375366\n",
            "Epoch: 28/200 Iteration: 3600 Loss: 1.1047208309173584\n",
            "Epoch: 29/200 Iteration: 3700 Loss: 0.9153419733047485\n",
            "Epoch: 30/200 Iteration: 3800 Loss: 0.9263482093811035\n",
            "Epoch: 31/200 Iteration: 3900 Loss: 1.0110433101654053\n",
            "Epoch: 31/200 Iteration: 4000 Loss: 0.9967666864395142\n",
            "b'the raven having Have things have no exit The Count Castiglione breathed its absolute limits of these roses an emotion, That lull\\'d should be read to thee to be a poem styled \"Romance\" Where tears of rare workmanship, and appears From Alfred With quietude, of it by no quiet mountain against the editor of a harp-string known spirit till his soul into me a wife and all the soul watched narrowly each I do they fell unto night half in the poet\\'s own poems Vulture, When, The learned you con Query \"fervor\"?--Ed.] There is this soul in writing in her mother found him'\n",
            "Epoch: 32/200 Iteration: 4100 Loss: 0.9624727964401245\n",
            "Epoch: 33/200 Iteration: 4200 Loss: 0.7654902935028076\n",
            "Epoch: 34/200 Iteration: 4300 Loss: 0.7653582692146301\n",
            "Epoch: 35/200 Iteration: 4400 Loss: 0.8406599760055542\n",
            "Epoch: 35/200 Iteration: 4500 Loss: 0.8432062864303589\n",
            "Epoch: 36/200 Iteration: 4600 Loss: 0.830623209476471\n",
            "Epoch: 37/200 Iteration: 4700 Loss: 0.7165158987045288\n",
            "Epoch: 38/200 Iteration: 4800 Loss: 0.6429605484008789\n",
            "Epoch: 39/200 Iteration: 4900 Loss: 0.8550044894218445\n",
            "Epoch: 39/200 Iteration: 5000 Loss: 0.74700528383255\n",
            "b'the raven had to its merit--in man man yet a start at full length-- And not again and even do to be made A truth of his poetry, we have known to Mrs. Poe, character, a tyranny that the night of Poetry for one or magazine terms on which the intention in Conscience side in jest-- so far as this mystery explore-- Let him that she lay to a kingdom of a similar manner. The measures suit-- did regard the marble and the wise was invariably required--first, some poems as not what now, in a poem of \"The Bells\" hath poetry and up'\n",
            "Epoch: 40/200 Iteration: 5100 Loss: 0.7268303632736206\n",
            "Epoch: 41/200 Iteration: 5200 Loss: 0.5973556637763977\n",
            "Epoch: 42/200 Iteration: 5300 Loss: 0.6313943266868591\n",
            "Epoch: 43/200 Iteration: 5400 Loss: 0.7154859900474548\n",
            "Epoch: 43/200 Iteration: 5500 Loss: 0.731423020362854\n",
            "Epoch: 44/200 Iteration: 5600 Loss: 0.6637169122695923\n",
            "Epoch: 45/200 Iteration: 5700 Loss: 0.5429820418357849\n",
            "Epoch: 46/200 Iteration: 5800 Loss: 0.5532003045082092\n",
            "Epoch: 47/200 Iteration: 5900 Loss: 0.6690291166305542\n",
            "Epoch: 47/200 Iteration: 6000 Loss: 0.6759441494941711\n",
            "b'the raven should be eye. To my name. already receiving me. upon her shrine On the saintly voice that whom thine absence and was the most wild character was impotent by the most legitimate manner, delicacy was now so excluded. as a copy at the public domain (does it grew ashen and for the skies, With my smiling anything I have wandered On yesterday he spoke to the Earth--and of poets. _Cas_. What time often each other\\'s functions at random. The old which at war in her pride. Alone have left may look upon every syllable! friends. slightly entirely many many are \"The'\n",
            "Epoch: 48/200 Iteration: 6100 Loss: 0.5651931166648865\n",
            "Epoch: 49/200 Iteration: 6200 Loss: 0.5327132940292358\n",
            "Epoch: 50/200 Iteration: 6300 Loss: 0.5613973140716553\n",
            "Epoch: 51/200 Iteration: 6400 Loss: 0.6563761830329895\n",
            "Epoch: 51/200 Iteration: 6500 Loss: 0.5525251030921936\n",
            "Epoch: 52/200 Iteration: 6600 Loss: 0.5438423752784729\n",
            "Epoch: 53/200 Iteration: 6700 Loss: 0.46051356196403503\n",
            "Epoch: 54/200 Iteration: 6800 Loss: 0.4936090111732483\n",
            "Epoch: 55/200 Iteration: 6900 Loss: 0.5442291498184204\n",
            "Epoch: 55/200 Iteration: 7000 Loss: 0.5450879335403442\n",
            "b'the raven having never deed must return no man ever down upon all alone Far Poe became unfitted to accept into the first, cast chains upon the sonorousness of the word, Pallas, itself. as a somewhat similar dreams I read, feel to interfere with the elementary tuition of \"Annabel in a thing a pale air, that lie some Some other in a deep green young vision Was neither man is again she their king. thou shalt be freely distributed \"The purpose, I would my friend, I would make Till secrecy and immediate hopes of those kisses That cumber is at will dwell-- Apart'\n",
            "Epoch: 56/200 Iteration: 7100 Loss: 0.5134241580963135\n",
            "Epoch: 57/200 Iteration: 7200 Loss: 0.4122432470321655\n",
            "Epoch: 58/200 Iteration: 7300 Loss: 0.47940829396247864\n",
            "Epoch: 59/200 Iteration: 7400 Loss: 0.5423238277435303\n",
            "Epoch: 59/200 Iteration: 7500 Loss: 0.5342180728912354\n",
            "Epoch: 60/200 Iteration: 7600 Loss: 0.5254836082458496\n",
            "Epoch: 61/200 Iteration: 7700 Loss: 0.37897542119026184\n",
            "Epoch: 62/200 Iteration: 7800 Loss: 0.4424397647380829\n",
            "Epoch: 63/200 Iteration: 7900 Loss: 0.5414830446243286\n",
            "Epoch: 63/200 Iteration: 8000 Loss: 0.4816966652870178\n",
            "b'the raven for their accuracy, it, full words on me, It reappeared had its perfume with her long days in tears and there has Some two more peculiar. Angelo, has been previously narrated. from ancient lore that in dreams I place at _all_ Howard made to involve or a replacement copy to thinking? But I saw thee and wine, Castiglione,--these cause been the idea which has so fearfully perished. Give the name I observed the man trembled the secret, especially as your sweet distant fire, As others in an \"old, bright, by the earth, but most singular assurance, had been thoroughly substantiated. To'\n",
            "Epoch: 64/200 Iteration: 8100 Loss: 0.5140097141265869\n",
            "Epoch: 65/200 Iteration: 8200 Loss: 0.41146400570869446\n",
            "Epoch: 66/200 Iteration: 8300 Loss: 0.43989211320877075\n",
            "Epoch: 67/200 Iteration: 8400 Loss: 0.5224793553352356\n",
            "Epoch: 67/200 Iteration: 8500 Loss: 0.4355561137199402\n",
            "Epoch: 68/200 Iteration: 8600 Loss: 0.43248677253723145\n",
            "Epoch: 69/200 Iteration: 8700 Loss: 0.3176882863044739\n",
            "Epoch: 70/200 Iteration: 8800 Loss: 0.37498384714126587\n",
            "Epoch: 71/200 Iteration: 8900 Loss: 0.4181899428367615\n",
            "Epoch: 71/200 Iteration: 9000 Loss: 0.4095176160335541\n",
            "b\"the raven was not a poem may bring it Have for such strange destruction matter with all things, to my It would loved by turns,-- trouble but most wild and general and well established, portion before I had a great measure, not written in 1848, to the harmony after light, And would the glory mournfully, And his hero fell, that the dark ages was not at her side, And whisper'd the winds a-weary best like leave it, such and so long with him, alas! My saw that I, of art should you on earth with anxieties It had no one above their literary\"\n",
            "Epoch: 72/200 Iteration: 9100 Loss: 0.36327672004699707\n",
            "Epoch: 73/200 Iteration: 9200 Loss: 0.2969960868358612\n",
            "Epoch: 74/200 Iteration: 9300 Loss: 0.31783342361450195\n",
            "Epoch: 75/200 Iteration: 9400 Loss: 0.4201487898826599\n",
            "Epoch: 75/200 Iteration: 9500 Loss: 0.3618495762348175\n",
            "Epoch: 76/200 Iteration: 9600 Loss: 0.41286155581474304\n",
            "Epoch: 77/200 Iteration: 9700 Loss: 0.31076622009277344\n",
            "Epoch: 78/200 Iteration: 9800 Loss: 0.35937508940696716\n",
            "Epoch: 79/200 Iteration: 9900 Loss: 0.49146518111228943\n",
            "Epoch: 79/200 Iteration: 10000 Loss: 0.49665069580078125\n",
            "b'the raven had bound by him Upon some spirit not awakening, the dread of a single short pause in brief, and these lines from this indeed very man more thoroughly authenticated. than he treated him now omitted, criticism as becomes made the special effects, thus thus burning Poe became forthwith in \"with poetry and enjoying perusal by our view it was in many the distance dim, upon my part in that every time having abandoned every other reappeared in \\'Graham\\'s death the pieces and in its black in Poetry and as well by its proper phantom in the analytic conception. in effect, as'\n",
            "Epoch: 80/200 Iteration: 10100 Loss: 0.4922274351119995\n",
            "Epoch: 81/200 Iteration: 10200 Loss: 0.3419124484062195\n",
            "Epoch: 82/200 Iteration: 10300 Loss: 0.4262009561061859\n",
            "Epoch: 83/200 Iteration: 10400 Loss: 0.4619097113609314\n",
            "Epoch: 83/200 Iteration: 10500 Loss: 0.4122985899448395\n",
            "Epoch: 84/200 Iteration: 10600 Loss: 0.48270106315612793\n",
            "Epoch: 85/200 Iteration: 10700 Loss: 0.292484313249588\n",
            "Epoch: 86/200 Iteration: 10800 Loss: 0.44395989179611206\n",
            "Epoch: 87/200 Iteration: 10900 Loss: 0.43581831455230713\n",
            "Epoch: 87/200 Iteration: 11000 Loss: 0.3616476356983185\n",
            "b'the raven on which had no may obtain a operations or shadow rested Upon no cost an shadowy time in some minds, those sable sad tears there was there to complying so at all tones! Then read is a constant music of natural there was entire in showing its Beneath a whole being of past such which in present had best to inquire by common to, this will of dark cankers is still by itself. to me where my chamber as shall make a great poet--the fool with me of glory! this much of late, The old of the ocean, As of one'\n",
            "Epoch: 88/200 Iteration: 11100 Loss: 0.4003983736038208\n",
            "Epoch: 89/200 Iteration: 11200 Loss: 0.32231852412223816\n",
            "Epoch: 90/200 Iteration: 11300 Loss: 0.3227630853652954\n",
            "Epoch: 91/200 Iteration: 11400 Loss: 0.38718292117118835\n",
            "Epoch: 91/200 Iteration: 11500 Loss: 0.3318166136741638\n",
            "Epoch: 92/200 Iteration: 11600 Loss: 0.3253600299358368\n",
            "Epoch: 93/200 Iteration: 11700 Loss: 0.2519363462924957\n",
            "Epoch: 94/200 Iteration: 11800 Loss: 0.3031546473503113\n",
            "Epoch: 95/200 Iteration: 11900 Loss: 0.33125588297843933\n",
            "Epoch: 95/200 Iteration: 12000 Loss: 0.32832494378089905\n",
            "b\"the raven But come o'er far-off seas, His re-entry into sad, solemn, a we must die too more against one as the sun was proud to be brilliancy such the poetical talent all had been with my fate and was these conversations, dear Monos; to those bygone happy will bring anything our poets, to be considered in their pale night-- and there reigned by no doubt, have at her burning unextinguished to take heaven, many before condemned--that Now this zeal I hear Upon that hear there were long as your years still her left her lips his tower, As one his knew while\"\n",
            "Epoch: 96/200 Iteration: 12100 Loss: 0.2969491481781006\n",
            "Epoch: 97/200 Iteration: 12200 Loss: 0.3099271059036255\n",
            "Epoch: 98/200 Iteration: 12300 Loss: 0.2714906930923462\n",
            "Epoch: 99/200 Iteration: 12400 Loss: 0.457200825214386\n",
            "Epoch: 99/200 Iteration: 12500 Loss: 0.3712869882583618\n",
            "Epoch: 100/200 Iteration: 12600 Loss: 0.35618188977241516\n",
            "Epoch: 101/200 Iteration: 12700 Loss: 0.3256922662258148\n",
            "Epoch: 102/200 Iteration: 12800 Loss: 0.34040960669517517\n",
            "Epoch: 103/200 Iteration: 12900 Loss: 0.4527807831764221\n",
            "Epoch: 103/200 Iteration: 13000 Loss: 0.44168218970298767\n",
            "b'the raven heart. I have not already And all the best as question well Hell, or what SILENCE. in dreams the bright cheek, of it that I might render be seen That is superior them in every day recollected; Look Be still!--it there was in extent, hath cost properly This glory to rest, he himself in the two ideas there was not by a few than \"Plain Vanilla This immediately in none, and a strange right enough. * for sense, naturally delighted in the poem, in Europe and made as he declared, is quadrated to a rare and radiant thoroughly dignified, for Heaven'\n",
            "Epoch: 104/200 Iteration: 13100 Loss: 0.42498475313186646\n",
            "Epoch: 105/200 Iteration: 13200 Loss: 0.36454373598098755\n",
            "Epoch: 106/200 Iteration: 13300 Loss: 0.3612850308418274\n",
            "Epoch: 107/200 Iteration: 13400 Loss: 0.41706132888793945\n",
            "Epoch: 107/200 Iteration: 13500 Loss: 0.3546071946620941\n",
            "Epoch: 108/200 Iteration: 13600 Loss: 0.36574387550354004\n",
            "Epoch: 109/200 Iteration: 13700 Loss: 0.2711810767650604\n",
            "Epoch: 110/200 Iteration: 13800 Loss: 0.30997055768966675\n",
            "Epoch: 111/200 Iteration: 13900 Loss: 0.33078235387802124\n",
            "Epoch: 111/200 Iteration: 14000 Loss: 0.31587618589401245\n",
            "b'the raven But alluded and to you have a lofty world when, the most true-- All very great through an earnestness, some of everybody had none. time upon the ear, in their hearts. and true-- For this last above, below, wind dared In all love The day long To be read in no cost and fly, among by the whole structure continually down--and having wherewith gone unto our God, in her heart. A domes--up spirit more of our proper subjection at time all what else to have been written early it is a personal matter--a forms, in the queries you thought, or him,'\n",
            "Epoch: 112/200 Iteration: 14100 Loss: 0.27705898880958557\n",
            "Epoch: 113/200 Iteration: 14200 Loss: 0.20416948199272156\n",
            "Epoch: 114/200 Iteration: 14300 Loss: 0.2214987576007843\n",
            "Epoch: 115/200 Iteration: 14400 Loss: 0.3063208758831024\n",
            "Epoch: 115/200 Iteration: 14500 Loss: 0.24585095047950745\n",
            "Epoch: 116/200 Iteration: 14600 Loss: 0.233458012342453\n",
            "Epoch: 117/200 Iteration: 14700 Loss: 0.14823585748672485\n",
            "Epoch: 118/200 Iteration: 14800 Loss: 0.1866006851196289\n",
            "Epoch: 119/200 Iteration: 14900 Loss: 0.2704768180847168\n",
            "Epoch: 119/200 Iteration: 15000 Loss: 0.2617254853248596\n",
            "b\"the raven and repeating the final destruction was the most being of all. little little which, from the limits of the wings Some which we if very night that one that the atmosphere alone, itself my hopes too, as not made the effect is at the utility. far wiser pardon of pleasure without a mad Like form for the beam contact which is did thought, too one a breathless and But, with similar stage,--that could no wind blew alone Is the gray rocks, that the final less was editor I saw he for him in an anchor'd realm, in phraseology in view he\"\n",
            "Epoch: 120/200 Iteration: 15100 Loss: 0.29367440938949585\n",
            "Epoch: 121/200 Iteration: 15200 Loss: 0.3709326684474945\n",
            "Epoch: 122/200 Iteration: 15300 Loss: 0.39831972122192383\n",
            "Epoch: 123/200 Iteration: 15400 Loss: 0.4824911952018738\n",
            "Epoch: 123/200 Iteration: 15500 Loss: 0.48058950901031494\n",
            "Epoch: 124/200 Iteration: 15600 Loss: 0.4813827872276306\n",
            "Epoch: 125/200 Iteration: 15700 Loss: 0.4185681939125061\n",
            "Epoch: 126/200 Iteration: 15800 Loss: 0.38744106888771057\n",
            "Epoch: 127/200 Iteration: 15900 Loss: 0.47363072633743286\n",
            "Epoch: 127/200 Iteration: 16000 Loss: 0.4656328558921814\n",
            "b\"the raven breathing From their soul, will now, purpose to hear there before the image more than thrones upon her noon, may ev'n company from time of tune, A sight lay in the gardens in keeping with the author's gardens I'll follow the sea, for his eBooks was passed into the grave; and format With a most Project Gutenberg-tm electronic works provided that Mr. W. despair; you, with knowledge Of laws by his words it passed into me flying might have proved upon every youth, and his could merely one that the day, having impressed me thus? her Night Upon her sweet golden\"\n",
            "Epoch: 128/200 Iteration: 16100 Loss: 0.3992236852645874\n",
            "Epoch: 129/200 Iteration: 16200 Loss: 0.38117560744285583\n",
            "Epoch: 130/200 Iteration: 16300 Loss: 0.3305108845233917\n",
            "Epoch: 131/200 Iteration: 16400 Loss: 0.3657352328300476\n",
            "Epoch: 131/200 Iteration: 16500 Loss: 0.29345250129699707\n",
            "Epoch: 132/200 Iteration: 16600 Loss: 0.2765998840332031\n",
            "Epoch: 133/200 Iteration: 16700 Loss: 0.15843380987644196\n",
            "Epoch: 134/200 Iteration: 16800 Loss: 0.24524453282356262\n",
            "Epoch: 135/200 Iteration: 16900 Loss: 0.26047781109809875\n",
            "Epoch: 135/200 Iteration: 17000 Loss: 0.18371406197547913\n",
            "b'the raven And who tolling, now unable so sweetly kissed happy eye, in tenure, or cannot be your dim cycles run could there will lead him to speak of my spirit as poets, the river was so long and profoundly, long a year of time it a reason save at this work, is my lot To the _d\\xc3\\xa9nouement_ proper--with which no thought that an idle longing word \"Nevermore\" at once too great because of all God\\'s providence like the flowers until a Raven, the wedding. \\'Tis name seemed above when our After length. the life of my yellow deceased again our tone of'\n",
            "Epoch: 136/200 Iteration: 17100 Loss: 0.20382119715213776\n",
            "Epoch: 137/200 Iteration: 17200 Loss: 0.14600153267383575\n",
            "Epoch: 138/200 Iteration: 17300 Loss: 0.21303804218769073\n",
            "Epoch: 139/200 Iteration: 17400 Loss: 0.27863067388534546\n",
            "Epoch: 139/200 Iteration: 17500 Loss: 0.28077825903892517\n",
            "Epoch: 140/200 Iteration: 17600 Loss: 0.2514623701572418\n",
            "Epoch: 141/200 Iteration: 17700 Loss: 0.20407234132289886\n",
            "Epoch: 142/200 Iteration: 17800 Loss: 0.3923245966434479\n",
            "Epoch: 143/200 Iteration: 17900 Loss: 0.4697979688644409\n",
            "Epoch: 143/200 Iteration: 18000 Loss: 0.4778206944465637\n",
            "b\"the raven having Have things in her meek and enduring vast are often why, thy bounds in another world--the and spirit-stirring, to he 'knew' 3. set a web of one about to the reciprocating Any not copy, a copy, then, this is some means each glory And let me of the proud watchful love. o'er but by his first volume. It pervades only both over all of whom my sad and there may one to this form Were Ah! in our high love in especial, hung off, or what is one made By this word 'judgment' for example, shall had thought in our\"\n",
            "Epoch: 144/200 Iteration: 18100 Loss: 0.5165500044822693\n",
            "Epoch: 145/200 Iteration: 18200 Loss: 0.4893702268600464\n",
            "Epoch: 146/200 Iteration: 18300 Loss: 0.4205411374568939\n",
            "Epoch: 147/200 Iteration: 18400 Loss: 0.543900191783905\n",
            "Epoch: 147/200 Iteration: 18500 Loss: 0.488951712846756\n",
            "Epoch: 148/200 Iteration: 18600 Loss: 0.4950964152812958\n",
            "Epoch: 149/200 Iteration: 18700 Loss: 0.3444257080554962\n",
            "Epoch: 150/200 Iteration: 18800 Loss: 0.3361409604549408\n",
            "Epoch: 151/200 Iteration: 18900 Loss: 0.4194166362285614\n",
            "Epoch: 151/200 Iteration: 19000 Loss: 0.22268563508987427\n",
            "b'the raven Have for her, In which it is you can but I remember rather with further aid they or at literary We do not claim by no wind. who are neither brute hither were a certain extent by having, Thus back on thy friend. _Bal_. Give me then, that any disclaimer of the darkness from their literary merits, are on the last moment--at my soul with shivered, That that the beam beauty and at or matter and a hand of his chamber door-- Perched in upon a huge happy face one in effect, have reached at last farewell he affected us the'\n",
            "Epoch: 152/200 Iteration: 19100 Loss: 0.24587935209274292\n",
            "Epoch: 153/200 Iteration: 19200 Loss: 0.1394885778427124\n",
            "Epoch: 154/200 Iteration: 19300 Loss: 0.20559188723564148\n",
            "Epoch: 155/200 Iteration: 19400 Loss: 0.2517486810684204\n",
            "Epoch: 155/200 Iteration: 19500 Loss: 0.15272268652915955\n",
            "Epoch: 156/200 Iteration: 19600 Loss: 0.20365267992019653\n",
            "Epoch: 157/200 Iteration: 19700 Loss: 0.13811556994915009\n",
            "Epoch: 158/200 Iteration: 19800 Loss: 0.18653498589992523\n",
            "Epoch: 159/200 Iteration: 19900 Loss: 0.19859129190444946\n",
            "Epoch: 159/200 Iteration: 20000 Loss: 0.14721569418907166\n",
            "b'the raven It exists _within their own condition To the first In the shadowy and most low up although not in a poem so little or immediate of their expression volume of that I feel at length. And love in that deep blush That I journeyed--I journeyed crucifix That a I love thee, thy most purple very every whose influence will be called tax deductible to leave them dear work or obtain permission as for a Project volunteers Project Gutenberg-tm, including turned that of an imp such long at which Colonel volumes has a owing at this self-torture, that I still the character'\n",
            "Epoch: 160/200 Iteration: 20100 Loss: 0.2188941389322281\n",
            "Epoch: 161/200 Iteration: 20200 Loss: 0.188974991440773\n",
            "Epoch: 162/200 Iteration: 20300 Loss: 0.245768204331398\n",
            "Epoch: 163/200 Iteration: 20400 Loss: 0.41521337628364563\n",
            "Epoch: 163/200 Iteration: 20500 Loss: 0.47329092025756836\n",
            "Epoch: 164/200 Iteration: 20600 Loss: 0.4969671964645386\n",
            "Epoch: 165/200 Iteration: 20700 Loss: 0.4860820174217224\n",
            "Epoch: 166/200 Iteration: 20800 Loss: 0.6384698152542114\n",
            "Epoch: 167/200 Iteration: 20900 Loss: 0.6587785482406616\n",
            "Epoch: 167/200 Iteration: 21000 Loss: 0.5318122506141663\n",
            "b'the raven How we grow I fill I am Nor the word, of those conveniences had yet their hearts. [Footnote 1: A diademed enshrouded. As he in either one his head between that in all philosophy So unto what of the best story. You not greatly in joy and drew The of more author. to his poem properly difficult before; By this may me, in joy as the field of Beauty, who knew And high way in regard him for some little of little volume, now left had written early upon my tomb. always appeared to those 1840 1845. * * * 25.'\n",
            "Epoch: 168/200 Iteration: 21100 Loss: 0.47504347562789917\n",
            "Epoch: 169/200 Iteration: 21200 Loss: 0.3396969139575958\n",
            "Epoch: 170/200 Iteration: 21300 Loss: 0.35293683409690857\n",
            "Epoch: 171/200 Iteration: 21400 Loss: 0.40384113788604736\n",
            "Epoch: 171/200 Iteration: 21500 Loss: 0.22723090648651123\n",
            "Epoch: 172/200 Iteration: 21600 Loss: 0.2334662675857544\n",
            "Epoch: 173/200 Iteration: 21700 Loss: 0.16676990687847137\n",
            "Epoch: 174/200 Iteration: 21800 Loss: 0.2413041591644287\n",
            "Epoch: 175/200 Iteration: 21900 Loss: 0.26735764741897583\n",
            "Epoch: 175/200 Iteration: 22000 Loss: 0.14231853187084198\n",
            "b'the raven and Were And the red bells! Of nothing more, flowers, and yet with no violet eye of her love is folly, and save that this home has no longer. Again his merry freaks Words my beautiful, my heart be How they were palsied There be known are floating in this lady, by the incident in all the whole of the night, neither brute How the stars shall happen, that no need eye; And my soul into me are accurately our design, it alone hear thy chamber-window, I see To the heart as they thunder by, I would have a great favorite'\n",
            "Epoch: 176/200 Iteration: 22100 Loss: 0.17629235982894897\n",
            "Epoch: 177/200 Iteration: 22200 Loss: 0.10294713079929352\n",
            "Epoch: 178/200 Iteration: 22300 Loss: 0.16444173455238342\n",
            "Epoch: 179/200 Iteration: 22400 Loss: 0.2049238234758377\n",
            "Epoch: 179/200 Iteration: 22500 Loss: 0.1139293983578682\n",
            "Epoch: 180/200 Iteration: 22600 Loss: 0.1400127112865448\n",
            "Epoch: 181/200 Iteration: 22700 Loss: 0.12594804167747498\n",
            "Epoch: 182/200 Iteration: 22800 Loss: 0.16578422486782074\n",
            "Epoch: 183/200 Iteration: 22900 Loss: 0.2664775252342224\n",
            "Epoch: 183/200 Iteration: 23000 Loss: 0.23742574453353882\n",
            "b'the raven How had called the passionate girl, Then most ignorant we Have slept with my heart. As the versification, although \"The Shepherd\\'s within a deep mourn red flashing fair lofty forest. One long I say could yet no more position a small and I, this poem will with it-- than thrones upon my love is folly, I before the us as granted by the mother having happy to me Upturned, voice, are no let its present pathway every spiritual ANNABEL LEE. and their burning glittering without possible merely the eyes still left no most attempt and its about one whose whom of'\n",
            "Epoch: 184/200 Iteration: 23100 Loss: 0.36058294773101807\n",
            "Epoch: 185/200 Iteration: 23200 Loss: 0.46798431873321533\n",
            "Epoch: 186/200 Iteration: 23300 Loss: 0.6065669059753418\n",
            "Epoch: 187/200 Iteration: 23400 Loss: 0.8865704536437988\n",
            "Epoch: 187/200 Iteration: 23500 Loss: 0.6649658679962158\n",
            "Epoch: 188/200 Iteration: 23600 Loss: 0.6953718662261963\n",
            "Epoch: 189/200 Iteration: 23700 Loss: 0.5125684142112732\n",
            "Epoch: 190/200 Iteration: 23800 Loss: 0.5335357785224915\n",
            "Epoch: 191/200 Iteration: 23900 Loss: 0.5410606861114502\n",
            "Epoch: 191/200 Iteration: 24000 Loss: 0.37388038635253906\n",
            "b\"the raven How love! whatever sir, still arise directly or seem Though its many days of our existence, objects saw he now, dismissing any mind no restrictions dream And weariness all excitements toppling hymns, Laves some poem, it weighed him into the sounds are of investigation * NOTE you received this will WARRANTIES OF EDGAR a second but more sonorous one of these feet, to have been a work was thus now trod perhaps, at her drowsy thing-- With the soul may allay the fancy whose (by which I would known; mistress or perceptible was music of his volumes at random)--'Of genius of\"\n",
            "Epoch: 192/200 Iteration: 24100 Loss: 0.36491823196411133\n",
            "Epoch: 193/200 Iteration: 24200 Loss: 0.29046306014060974\n",
            "Epoch: 194/200 Iteration: 24300 Loss: 0.2662017345428467\n",
            "Epoch: 195/200 Iteration: 24400 Loss: 0.3752409815788269\n",
            "Epoch: 195/200 Iteration: 24500 Loss: 0.26331016421318054\n",
            "Epoch: 196/200 Iteration: 24600 Loss: 0.2591777443885803\n",
            "Epoch: 197/200 Iteration: 24700 Loss: 0.16708986461162567\n",
            "Epoch: 198/200 Iteration: 24800 Loss: 0.1798420548439026\n",
            "Epoch: 199/200 Iteration: 24900 Loss: 0.22035211324691772\n",
            "Epoch: 199/200 Iteration: 25000 Loss: 0.12259213626384735\n",
            "b\"the raven having breathing I'll be,'mid perceive that its name. It would have been looked surprising, was understood the ring in its meaning in dreams we have encountered I not tell at my flight-- soul grew feebler She lately savored Information about her combinations shall be copied about his flower a winds in like the pallid While even the long, long the deep rest, At the poetical God's We were creation, or what the less name before, will half not do endure to your sweet soul in a somewhat TO The ring of married _Di My eyes in quadruple one movement of this\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlOZp-euu1xk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(net.state_dict(),'./data/model-{}.pth'.format(200))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnJSNI1H9_yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}